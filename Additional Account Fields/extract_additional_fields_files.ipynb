{"cells":[{"cell_type":"markdown","source":["# Intro\n","This notebook takes uploaded files from Mythic, processes them and writes them to bronze as delta tables. It then moves the files to a processed folder to keep the uploaded folder open for the next batch of files.  \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74fdd95f-5ae8-4aa0-97b9-4dbc350157f5"},{"cell_type":"markdown","source":["\n","## Change History\n","\n","<style>\n","  table {margin-left: 0 !important;}\n","</style>\n","\n","| Date    | Author | Description |\n","| :-------- | :------- | :------- | \n","|2025-02-12 | Mclain R |  Create Date|"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56a1706b-7688-4e40-9a50-12db24525e5a"},{"cell_type":"markdown","source":["# Code"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"59148fe2-465a-4244-8d6f-79478ec3c8e6"},{"cell_type":"markdown","source":["## Imports\n","\n","###### notebookutils\n","- **mssparkutils**: A utility module in Microsoft Fabric that provides functions for handling file operations, secrets, and other notebook-related tasks within the Spark environment.\n","\n","###### python\n","- **re**: The regular expressions module used for pattern matching, text parsing, and string manipulation in Python.\n","\n","###### sempy\n","- **fabric**: Part of the Semantic Link (SEMpy) library, sempy.fabric provides tools for interacting with Microsoft Fabric, enabling operations such as querying semantic models, working with datasets, and integrating with Fabric's data services. It's commonly used for extracting insights from Power BI datasets and semantic models within Fabric."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8529de85-aa64-471d-b7e6-2ed46cafc21f"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","import re\n","import sempy\n","import sempy.fabric as fabric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc9e348b-5262-4af6-b172-b300ffb67829"},{"cell_type":"markdown","source":["## Define Parameters\n","- none\n","\n","Note: the following is a parameter cell and will be interpreted by Pipelines as such."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f800370-3165-4827-b339-8587dc6866c8"},{"cell_type":"markdown","source":["## Reused Functions\n","- none"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9a76fe4-f205-4829-81d9-459a609fe7e7"},{"cell_type":"markdown","source":["## Define Fields\n","\n","- **workspace_name**: name of workspace\n","- **client_name**: name of client. Used to name adls shortcut\n","- **adls_folder**: name of adls shortcutted folder\n","- **destination_folder**: name of folder to write to\n","- **directory_path**: path of adls storage\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2cbb0aa3-3e48-4ca3-85d4-ff313f12babe"},{"cell_type":"code","source":["# Get the current workspace ID\n","workspace_id = fabric.get_workspace_id()\n","print(f\"Workspace ID: {workspace_id}\")\n","\n","# Get the workspace name from the workspace ID\n","workspace_name = fabric.resolve_workspace_name(workspace_id)\n","print(f\"Workspace Name: {workspace_name}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"357fa03a-d9a9-4d7c-8d2d-88999ec8d201"},{"cell_type":"code","source":["client_name = \"mythic\"\n","adls_folder = \"uploader\"\n","destination_folder = \"processed\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b125e7f8-7869-487b-afb1-b30253be009a"},{"cell_type":"code","source":["# Define the directory path in your ADLS Gen2 storage\n","directory_path = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/bronze_lakehouse.Lakehouse/Files/adls_{client_name}/{adls_folder}\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40d5179c-e4b3-40eb-9712-21adf4949e40"},{"cell_type":"markdown","source":["## Process Data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e6a92e02-7ae2-408d-88ed-03369299bcad"},{"cell_type":"markdown","source":["3/26/24 MR added Check headers for correct schema"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31a6e037-283a-4cd8-9ae6-16f473032bbe"},{"cell_type":"markdown","source":["##### dynamically get all csv files in the uploader folder and write them to bronze delta tables. Then move files from uploader folder to processed folder."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d2f7edd-50c0-4476-9e12-b35ab315aa2e"},{"cell_type":"code","source":["# Get list of files in the directory\n","file_list = mssparkutils.fs.ls(directory_path)\n","\n","for file in file_list:\n","    # Check if it's a CSV file and starts with the desired prefix\n","    if file.name.endswith(\".csv\") and (file.name.startswith(\"ICRM Additional Account Fields_\") or file.name.startswith(\"DCRM Additional Account Fields_\")):\n","        # Extract table name from file name and convert to lowercase\n","        table_name = file.name.split(\"_\")[0].lower()\n","\n","        # Replace spaces with underscores in the table name\n","        table_name = table_name.replace(\" \", \"_\")\n","        \n","        # Read CSV file with options to handle special characters and double quotes correctly\n","        df = spark.read.csv(\n","            file.path, \n","            header=True, \n","            inferSchema=True\n","        )\n","        \n","        #convert camelcase to snakecase and clean up headers\n","        df = df.toDF(*(c.lower() for c in df.columns))\n","        df = df.toDF(*(c.replace('&','_') for c in df.columns))\n","        df = df.toDF(*(c.replace(' ','_') for c in df.columns))\n","        df = df.toDF(*(c.replace('.','_') for c in df.columns))\n","        df = df.toDF(*(c.replace('-','_') for c in df.columns))\n","        df = df.toDF(*(c.replace('/','_') for c in df.columns))\n","        df = df.toDF(*(c.replace(',','_') for c in df.columns))\n","        df = df.toDF(*(c.replace(';','_') for c in df.columns))\n","        df = df.toDF(*(c.replace(')','') for c in df.columns))\n","        df = df.toDF(*(c.replace('(','') for c in df.columns))\n","        \n","        # Define expected cleaned schema for each file type\n","        expected_schemas = {\n","            'icrm_additional_account_fields': ['bac_code', 'parent_bac', 'parent_account', 'pdn',\n","            'account_dba_name', 'account_id', 'dealer_group_code', 'dealer_group_name', 'dealer_status',\n","            'garage_package', 'garage_current_provider', 'f_i_relationship', 'f_i_dir_of_sales',\n","            'region_description', 'uniqueaccountid', 'slp', 'primary_vsc_provider', 'floorplan', 'af_retail',\n","            'vsc_vol__3mo', 'gap_vol__3mo', 'otherfi_vol__3mo', 'product_density_3mo', '31_day_avg__vol',\n","            'cur__31_day_vol', '%_diff_', 'call_reports', 'most_recent_call_date', 'slc_report', 'most_recent_slc_date',\n","            'product_density', 'floorplan_lost', 'paid_off_cash_advance', 'incentive', 'reinsurance', 'risk_score'],\n","            \n","            'dcrm_additional_account_fields': [\n","                'oe_id', 'account_name', 'account_id', 'parent_account', 'group_id',\n","                'dealer_status', 'group_name', 'privileges', 'sub_privileges', 'uniqueaccountid'\n","            ]\n","        }\n","\n","        # Get expected columns for this table\n","        expected_columns = expected_schemas.get(table_name)\n","        actual_columns = df.columns\n","\n","        # Fail if actual schema doesn't match expected\n","        if set(expected_columns) != set(actual_columns):\n","            raise Exception(\n","                f\"Schema mismatch in file {file.name}.<br><br>\"\n","                f\"Expected: {expected_columns}.<br><br>\"\n","                f\"Found: {actual_columns}.\"\n","            )\n","\n","        # Define Delta table path\n","        delta_table_path = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/bronze_lakehouse.Lakehouse/Tables/nucleus__{table_name}\"\n","\n","        # Write DataFrame to Delta table\n","        df.write.mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .format(\"delta\") \\\n","            .save(delta_table_path)\n","\n","        display(df.limit(10))\n","\n","        # Define path for uploaded file to move to\n","        destination_path = f\"abfss://Mythic@onelake.dfs.fabric.microsoft.com/bronze_lakehouse.Lakehouse/Files/{destination_folder}/{file.name}\"\n","\n","        # Use mssparkutils to copy the file\n","        mssparkutils.fs.cp(file.path, destination_path, recurse=True)\n","\n","        # Optionally, delete the original file after copying\n","        mssparkutils.fs.rm(file.path, recurse=True)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"bd0fc89a-d062-4b50-811d-23c0e971b999"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"9629524c-13f0-4a9a-affe-58ab1c2f5dbe","default_lakehouse_name":"bronze_lakehouse","default_lakehouse_workspace_id":"fef507b4-c0af-40cc-9309-b183e59c0547"}}},"nbformat":4,"nbformat_minor":5}